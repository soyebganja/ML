# -*- coding: utf-8 -*-
"""Ensemble_learning_Exercise2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUDZ22Ixw8LsC71RX-IrbLS5-8U83u_c

### Problem Statement

You are a data scientist / AI engineer working on a binary classification problem. You have been provided with a dataset named **`"mushroom_classification.csv"`**, which includes various features of mushrooms to predict whether they are edible or poisonous. The dataset comprises the following columns:

- `cap_diameter:` The diameter of the mushroom cap.
- `cap_shape:` The shape of the mushroom cap, encoded as integers.
- `gill_attachment:` The attachment of the gills, encoded as integers.
- `gill_color:` The color of the gills, encoded as integers.
- `stem_height:` The height of the mushroom stem.
- `stem_width` The width of the mushroom stem.
- `stem_color:` The color of the mushroom stem, encoded as integers.
- `season:` The season when the mushroom was found, encoded as integers.
- `class:` The classification of the mushroom, where 0 indicates edible and 1 indicates poisonous.

Your task is to use this dataset to build and evaluate a binary classification model to classify mushrooms as edible or poisonous. You will start with basic models and gradually move towards advanced models like Gradient Boosting. Finally, you will explore various parameters of the Gradient Boosting model to enhance performance.

**Dataset credits:** Prisha Sawhney (https://www.kaggle.com/datasets/prishasawhney/mushroom-dataset/data)

**Import Necessary Libraries**
"""

#import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report

"""### Task 1: Data Preparation and Exploration

1. Import the data from the `"mushroom_classification.csv"` file and store it in a variable df.
2. Display the number of rows and columns in the dataset.
3. Display the first few rows of the dataset to get an overview.
"""

# Step 1: Import the data from the "mushroom_classification.csv" file and store it in a variable 'df'
df = pd.read_csv("mushroom_classification.csv")

# Step 2: Display the number of rows and columns in the dataset
print(df.shape)

# Step 3: Display the first few rows of the dataset to get an overview
df.head()

"""### Task 2: Exploratory Data Analysis (EDA)

1. Perform a group-by operation on the target class and calculate the mean of the following features: `cap_diameter, stem_height, and stem_width`.
2. Visualize the distribution of these features using box plots.
"""

# Step 1: Perform a group-by operation on the target class and calculate the mean of specific features
df.groupby("class")[["cap_diameter", "stem_height", "stem_width"]].mean()

# Step 2: Visualize the distribution of these features using box plots
df.boxplot(column=["cap_diameter", "stem_height", "stem_width"], by="class")
plt.show()

"""### Task 3: Model Training Using Basic Models

1. Select the features `(cap_diameter, cap_shape, gill_attachment, gill_color, stem_height, stem_width, stem_color, season)` and the target variable `(class)` for modeling.
2. Split the data into training and test sets with a test size of 25%.
3. Initialize and train a Logistic Regression model using the training data.
4. Make predictions on the test set using the trained model.
5. Evaluate the model using a classification report and print the report.
6. Initialize and train a Decision Tree Classifier model using the training data.
7. Make predictions on the test set using the trained model.
8. Evaluate the model using a classification report and print the report.
"""

# Step 1: Select the features and target variable for modeling
x = df[["cap_diameter", "cap_shape", "gill_attachment", "gill_color", "stem_height", "stem_width", "stem_color", "season"]]
y = df["class"]

# Step 2: Split the data into training and test sets with a test size of 25%
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# Step 3: Initialize and train a Logistic Regression model using the training data
model_lr = LogisticRegression(max_iter=700)
model_lr.fit(X_train, y_train)

# Step 4: Make predictions on the test set using the trained model
y_pred_lr = model_lr.predict(X_test)

# Step 5: Evaluate the model using a classification report and print the report
print("Logistic Regression Report: ")
print(classification_report(y_test, y_pred_lr))

# Step 6: Initialize and train a Decision Tree Classifier model using the training data
model_dtc = DecisionTreeClassifier()
model_dtc.fit(X_train, y_train)

# Step 7: Make predictions on the test set using the trained model
y_pred_dtc = model_dtc.predict(X_test)

# Step 8: Evaluate the model using a classification report and print the report
print(f"Decision Tree Classifier report: ")
print(classification_report(y_test, y_pred_dtc))

"""### Task 4: Model Training Using Gradient Boosting Classifier

1. Initialize and train a Gradient Boosting Classifier model using the training data.
2. Make predictions on the test set using the trained model.
3. Evaluate the model using a classification report and print the report.
4. Calculate and display the feature importances.
"""

# Step 1: Initialize and train a Gradient Boosting Classifier model using the training data
model_gbc = GradientBoostingClassifier()
model_gbc.fit(X_train, y_train)

# Step 2: Make predictions on the test set using the trained model
y_pred_gbc = model_gbc.predict(X_test)

# Step 3: Evaluate the model using a classification report and print the report
print(f"Gradient Boosting Classifier report: ")
print(classification_report(y_test, y_pred_gbc))

# Step 4: Calculate and display the feature importances
feature_importances = model_gbc.feature_importances_
feature_names = X_train.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
feature_importance_df

"""### Task 5: Exploring Various Parameters in Gradient Boosting Classifier

1. Train a Gradient Boosting model with the following parameters:
    - learning_rate = 0.05
    - n_estimators = 150
    - max_depth=4
    - min_samples_split = 3
    - min_samples_leaf = 2

Learn about these parameters here: [scikit-learn GradientBoostingClassifier Parameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)

-------------------------------------------------------------------------------------------------------------------------------------------------------
2. Evaluate the model using a classification report and print the report.
"""

# Step 1: Train a Gradient Boosting model with specified parameters
model_gbc_tuned = GradientBoostingClassifier(learning_rate=0.05, n_estimators=150, max_depth=4, min_samples_split=3, min_samples_leaf=2)
model_gbc_tuned.fit(X_test, y_test)

# Step 2: Make predictions on the test set using the trained model
y_pred_gbc_tuned =  model_gbc_tuned.predict(X_test)

# Step 3: Evaluate the model using a classification report and print the report
print(f"Tuned Gradient Boosting Classifier report: ")
print(classification_report(y_test, y_pred_gbc_tuned))

